12:58:16 INFO Training with 
{   'attention_mode': 'label',
    'batch_size': 8,
    'best_model_path': None,
    'bidirectional': 1,
    'd_a': 512,
    'dropout': 0.3,
    'embedding_file': 'data/embeddings/word2vec_sg0_100.model',
    'embedding_mode': 'word2vec',
    'embedding_size': 100,
    'exp_name': 'gcn_single_sum',
    'hidden_size': 512,
    'joint_mode': 'hierarchical',
    'level_projection_size': 128,
    'loss_name': 'DICE_LOSS11',
    'lr': 0.001,
    'lr_scheduler_factor': 0.9,
    'lr_scheduler_patience': 5,
    'main_metric': 'micro_f1',
    'max_seq_length': 4000,
    'metric_level': 1,
    'min_seq_length': -1,
    'min_word_frequency': -1,
    'mode': 'static',
    'model': <class 'src.models.rnn.RNN'>,
    'multilabel': 1,
    'n_epoch': 1,
    'n_layers': 1,
    'optimiser': 'adamw',
    'patience': 5,
    'penalisation_coeff': 0.01,
    'problem_name': 'mimic-iii_2_50',
    'r': -1,
    'reduction': 'sum',
    'resume_training': False,
    'rnn_model': 'LSTM',
    'save_best_model': 1,
    'save_results': 1,
    'save_results_on_train': False,
    'shuffle_data': 1,
    'use_last_hidden_state': 0,
    'use_lr_scheduler': 1,
    'use_regularisation': False,
    'weight_decay': 0}

12:58:16 INFO Loaded vocab and data from file
12:58:16 INFO Using cuda:2
12:58:16 INFO # levels: 2
12:58:16 INFO # labels at level 0: 34
12:58:16 INFO # labels at level 1: 44
12:58:16 INFO 5.5.5
12:58:18 INFO Saved dataset path: cached_data/mimic-iii_2_50/797d28c2de5595caf911a3ed71f510b1.data.pkl
12:58:18 INFO 5 instances with 19926 tokens, Level_0 with 26 labels, Level_1 with 30 labels in the train dataset
12:58:18 INFO 5 instances with 20000 tokens, Level_0 with 20 labels, Level_1 with 25 labels in the valid dataset
12:58:18 INFO 5 instances with 20000 tokens, Level_0 with 20 labels, Level_1 with 25 labels in the test dataset
12:58:18 INFO Training epoch #1
12:58:19 INFO Learning rate at epoch #1: 0.001
12:58:19 INFO Loss on Train at epoch #1: 0.68783, micro_f1 on Valid: 0.23377
12:58:19 INFO [NEW BEST] (level_1) micro_f1 on Valid set: 0.23377
12:58:19 INFO Results on Valid set at epoch #1 with Averaged Loss 0.75777
12:58:19 INFO ======== Results at level_0 ========
12:58:19 INFO Results on Valid set at epoch #1 with Loss 0.73772: 
[MICRO]	accuracy: 0.15556	auc: 0.5867	precision: 0.18667	recall: 0.48276	f1: 0.26923	P@1: 0	P@5: 0	P@8: 0	P@10: 0	P@15: 0
[MACRO]	accuracy: 0.11471	auc: 0.48333	precision: 0.11471	recall: 0.26471	f1: 0.16005	P@1: 0.4	P@5: 0.24	P@8: 0.2	P@10: 0.22	P@15: 0.2

12:58:19 INFO ======== Results at level_1 ========
12:58:19 INFO Results on Valid set at epoch #1 with Loss 0.77515: 
[MICRO]	accuracy: 0.13235	auc: 0.51084	precision: 0.14634	recall: 0.58065	f1: 0.23377	P@1: 0	P@5: 0	P@8: 0	P@10: 0	P@15: 0
[MACRO]	accuracy: 0.08864	auc: 0.45667	precision: 0.08864	recall: 0.31818	f1: 0.13865	P@1: 0.2	P@5: 0.16	P@8: 0.175	P@10: 0.2	P@15: 0.17333

12:58:19 INFO =================== BEST ===================
12:58:19 INFO Results on Valid set at epoch #1 with Averaged Loss 0.75777
12:58:19 INFO ======== Results at level_0 ========
12:58:19 INFO Results on Valid set at epoch #1 with Loss 0.73772: 
[MICRO]	accuracy: 0.15556	auc: 0.5867	precision: 0.18667	recall: 0.48276	f1: 0.26923	P@1: 0	P@5: 0	P@8: 0	P@10: 0	P@15: 0
[MACRO]	accuracy: 0.11471	auc: 0.48333	precision: 0.11471	recall: 0.26471	f1: 0.16005	P@1: 0.4	P@5: 0.24	P@8: 0.2	P@10: 0.22	P@15: 0.2

12:58:19 INFO ======== Results at level_1 ========
12:58:19 INFO Results on Valid set at epoch #1 with Loss 0.77515: 
[MICRO]	accuracy: 0.13235	auc: 0.51084	precision: 0.14634	recall: 0.58065	f1: 0.23377	P@1: 0	P@5: 0	P@8: 0	P@10: 0	P@15: 0
[MACRO]	accuracy: 0.08864	auc: 0.45667	precision: 0.08864	recall: 0.31818	f1: 0.13865	P@1: 0.2	P@5: 0.16	P@8: 0.175	P@10: 0.2	P@15: 0.17333

